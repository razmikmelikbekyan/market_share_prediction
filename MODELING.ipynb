{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "\n",
    "We will add some more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.special import inv_boxcox, boxcox1p\n",
    "from pprint import pprint\n",
    "\n",
    "# supress unnecessary warnings for readability and cleaner presentation\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function which performs all steps of data processing which we applied during \n",
    "# Data cleaning and preprocessing\n",
    "def read_clean_data():\n",
    "    \n",
    "    # reading data\n",
    "    df_train = pd.read_csv('use_case_data.csv')\n",
    "    df_score = pd.read_csv('score_data.csv')\n",
    "\n",
    "    df_train['data_type'] = 'train'\n",
    "    df_score['data_type'] = 'score'\n",
    "    \n",
    "    # removing negative market shares\n",
    "    temp = df_train[df_train['Market_Share'] >= 0]\n",
    "    \n",
    "    # applying boxcox\n",
    "    market_share = stats.boxcox(temp['Market_Share'].values + 1e-30, lmbda=0.25)\n",
    "    \n",
    "    # combining train and score\n",
    "    data = pd.concat([temp.drop(columns='Market_Share'), df_score],ignore_index=True)\n",
    "    \n",
    "    # adding response variable\n",
    "    data['Market_Share'] = np.nan\n",
    "    mask = data['data_type'] == 'train'\n",
    "    data.loc[mask, 'Market_Share'] = market_share\n",
    "    data.drop(columns='data_type', inplace=True)\n",
    "    \n",
    "    # changing dtype to category\n",
    "    data['ITEMSCODE'] = data['ITEMSCODE'].astype('category')\n",
    "\n",
    "    # spliting to year and month, adding to not_useful_features list\n",
    "    data['LAUNCH_YEAR'], data['LAUNCH_MONTH'] = data['NPLLAUNCHDATE'].map(str).apply(lambda x: [x[:4], x[4:]]).str   \n",
    "    \n",
    "    # removing not useful features\n",
    "    not_useful = ['BRMID', 'LATESTPERIODINDEX', 'NPLLAUNCHDATE', 'ISREPLACEMENT', 'BRM', 'MARKETEDBRAND',\n",
    "                  'BRANDSUBFAMILY', 'NPLLAUNCHYEAR', 'RTYPE', 'ITEMSHAPE']\n",
    "    \n",
    "    data.drop(columns=not_useful, inplace=True)\n",
    "    \n",
    "    # filling missing values\n",
    "    data['SPECIALFLAVOR'].fillna('NOSPECIALFLAVOR', inplace=True)\n",
    "    data['TIPCOLOR'].fillna('NOTIPCOLOR', inplace=True)\n",
    "    \n",
    "    # transform categorical features into the appropriate type\n",
    "    for c in data.columns:\n",
    "        col_type = data[c].dtype\n",
    "        if col_type == 'object' or col_type.name == 'category':\n",
    "            data[c] = data[c].astype('category')\n",
    "            \n",
    "    # transforming with boxcox1p for reducing skew\n",
    "    data[['LEN', 'NCON', 'RETAILPACKPRICE']] = boxcox1p(data[['LEN', 'NCON', 'RETAILPACKPRICE']], -0.25)\n",
    "\n",
    "    print('all data shape: {}'.format(data.shape))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add some more features by aggregating some categorical features by numeric ones.\n",
    "\n",
    "For example, we will calculate **mean, median, std, skew** of **RETAILPACKPRICE** for each **REGION**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(data, f_1, f_2):\n",
    "    \"\"\"Aggregates f_1 by f_2\"\"\"\n",
    "\n",
    "    mean_ = '{}_{}_mean'.format(f_1, f_2)\n",
    "    median_ = '{}_{}_median'.format(f_1, f_2)\n",
    "    std_ = '{}_{}_std'.format(f_1, f_2)\n",
    "    skew_ = '{}_{}_skew'.format(f_1, f_2)\n",
    "    \n",
    "    # aggregation of f_1 by f_2\n",
    "    temp = data.groupby(f_1)[f_2].aggregate({\n",
    "        mean_: np.mean,\n",
    "        std_: np.std,\n",
    "        median_: np.median,\n",
    "        skew_: stats.skew\n",
    "    }).reset_index()\n",
    "    \n",
    "    # filling nans with 0\n",
    "    for x in (mean_, std_, median_, skew_):\n",
    "        temp[x].fillna(0, inplace=True)\n",
    "        \n",
    "    data = data.merge(temp)\n",
    "    \n",
    "    # adding relative (for example product RETAILPACKPRICE / REGION mean RETAILPACKPRICE)\n",
    "    relative_mean_ = '{}_{}_relative_mean'.format(f_1, f_2)\n",
    "    relative_median_ = '{}_{}_relative_median'.format(f_1, f_2)\n",
    "    \n",
    "    def relative(row, f):\n",
    "        f_value, f_2_value = row[f], row[f_2]\n",
    "        if f_value == 0 and f_2_value == 0:\n",
    "            return 1.\n",
    "        elif f_value == 0 and f_2_value != 0:\n",
    "            return -99999\n",
    "        else:\n",
    "            return f_2_value / f_value\n",
    "        \n",
    "#     data[relative_mean_] = data.apply(lambda row: relative(row, mean_), axis=1)\n",
    "    data[relative_median_] = data.apply(lambda row: relative(row, median_), axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data, to_drop=None, to_aggregate=None, to_dummy=False, cardinality_ratio=1., ):\n",
    "    \"\"\"\n",
    "    Recives cleaned and preprocessed data, add features, high cardinality features and makes one-hot encoding.\n",
    "    \"\"\"\n",
    "    temp = data.copy()\n",
    "\n",
    "    # removing high cardinality categorical features\n",
    "    if cardinality_ratio < 1.:\n",
    "        n = len(temp)\n",
    "        high_cardinalty_features = []\n",
    "        for c in temp.columns:\n",
    "            if temp[c].dtype.name == 'category' and len(temp[c].unique()) / n > cardinality_ratio:\n",
    "                high_cardinalty_features.append(c)\n",
    "        print('Removing {} features, which have high cardinality.'.format(high_cardinalty_features))\n",
    "        temp.drop(columns=high_cardinalty_features, inplace=True)\n",
    "    \n",
    "    # aggergation\n",
    "    numeric_fs = [c for c in temp.columns if temp[c].dtype.name != 'category' and c != 'Market_Share']\n",
    "    for f_1 in to_aggregate:\n",
    "        for f_2 in numeric_fs:\n",
    "            temp = aggregate(temp, f_1, f_2)\n",
    "    \n",
    "    # adding also **2, **3 and **0.5 for LEN and RETAILPACKPRICE\n",
    "    for x in ['LEN', 'RETAILPACKPRICE']:\n",
    "        temp['{}_**2'.format(x)] = temp[x] ** 2\n",
    "        temp['{}_**3'.format(x)] = temp[x] ** 2\n",
    "        temp['{}_**0.5'.format(x)] = np.sqrt(np.abs(temp[x]))        \n",
    "    print('all data shape after features addition: {}'.format(temp.shape))\n",
    "    \n",
    "    # dropping some features\n",
    "    if to_drop:\n",
    "        temp.drop(columns=to_drop, inplace=True)\n",
    "        print('all data shape after features deletion: {}'.format(temp.shape))\n",
    "    \n",
    "\n",
    "    # one-hot encoding for categorical features\n",
    "    if to_dummy:\n",
    "        temp = pd.get_dummies(temp)\n",
    "        print('all data shape after one-hot-encoding: {}'.format(temp.shape))\n",
    "\n",
    "    # splitting into train and score\n",
    "    mask = temp['Market_Share'].notnull()\n",
    "    training_data, score_data = temp[mask], temp[np.invert(mask)]\n",
    "    score_data.drop(columns='Market_Share', inplace=True)\n",
    "    print('training data shape: {}, score data shape: {}'.format(\n",
    "        training_data.shape, score_data.shape))\n",
    "    return training_data, score_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing data: data clening, preprocessing and feature engineering, one-hot-encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all data shape: (1721, 32)\n"
     ]
    }
   ],
   "source": [
    "df = read_clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all data shape after features addition: (1721, 198)\n",
      "all data shape after features deletion: (1721, 193)\n",
      "all data shape after one-hot-encoding: (1721, 517)\n",
      "training data shape: (1692, 517), score data shape: (29, 516)\n"
     ]
    }
   ],
   "source": [
    "# categorical features, which will be aggregated\n",
    "to_aggregate = [\n",
    "    'TCLASS',\n",
    "    'REGION',\n",
    "    'LOCALCLASS',\n",
    "    'BLDIMAGE', \n",
    "    'MARKET',\n",
    "    'MINDICATOR', \n",
    "    'THICATEGORY', \n",
    "    'PCKT',\n",
    "]\n",
    "\n",
    "# categorical features, which will be dropped\n",
    "to_drop = [\n",
    "    'BRANDSUBFAMILYGROUPING', \n",
    "    'BRANDSUBFAMILYLINE',\n",
    "    'BRANDSUBFAMILYGROUP', \n",
    "    'BRANDONMARKET',\n",
    "    'BRANDDIFFERENTIATOR',\n",
    "]\n",
    "\n",
    "training_df, score_df = feature_engineering(df, \n",
    "                                            to_drop=to_drop, \n",
    "                                            to_aggregate=to_aggregate,\n",
    "                                            to_dummy=True, \n",
    "                                            cardinality_ratio=1.)\n",
    "\n",
    "numeric_fs = [c for c in training_df.columns if training_df[c].dtype.name != 'category' and c != 'Market_Share']\n",
    "categoircal_fs = list(set(training_df.columns) - set(numeric_fs) - {'Market_Share'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score, r2_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, confusion_matrix, make_scorer\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(matrix, classes, cmap=plt.cm.Reds):\n",
    "    \"\"\"This function plots the normalized confusion matrix.\"\"\"\n",
    "    matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(matrix, interpolation='nearest', cmap=cmap)\n",
    "    plt.title('Normalized Confusion matrix', fontsize=17)\n",
    "    cb = plt.colorbar()\n",
    "    cb.ax.tick_params(labelsize=15)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, fontsize=12)\n",
    "    plt.yticks(tick_marks, classes, fontsize=12)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = matrix.max() / 2.\n",
    "    for i, j in itertools.product(range(matrix.shape[0]),range(matrix.shape[1])):\n",
    "        plt.text(j, i, format(matrix[i, j], fmt),\n",
    "                 horizontalalignment=\"center\", fontsize=17,\n",
    "                 color=\"blue\" if matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=12)\n",
    "    plt.xlabel('Predicted label', fontsize=12)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_actual_vs_pred(y_true, y_pred, figure=True, show=True):\n",
    "    \"\"\"Plots model predictions vs their actual values.\"\"\"\n",
    "    if figure:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    plt.scatter(y_true, y_pred)\n",
    "    plt.title('Actual values vs Predicted values.')\n",
    "    plt.ylabel('Predicted', fontsize=12)\n",
    "    plt.xlabel('Actual', fontsize=12)\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "def plot_roc_curve(y_true, y_pred, figure=True, show=True, label=None):\n",
    "    \"\"\"Plots ROC curve.\"\"\"\n",
    "    \n",
    "    threshold = 0.007\n",
    "    threshold_transformed = stats.boxcox(threshold, 0.25)\n",
    "    binary_target = y_true > threshold_transformed\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(binary_target, y_pred)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    \n",
    "    if figure:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    temp = 'ROC (AUC = {:.2f})'.format(auc_score)\n",
    "    label = '{}: {}'.format(label, temp) if label else temp\n",
    "\n",
    "    plt.plot(fpr, tpr, lw=3, alpha=0.3, label=label)\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.title('ROC curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "def plot_model_performance(y_true, y_pred, matrix):\n",
    "    \"\"\"\n",
    "    This function plots:\n",
    "        - predicted values vs actual values\n",
    "        - confusion matrix\n",
    "        - roc curve\n",
    "    \"\"\"\n",
    "    print()\n",
    "    # plotting predicted values vs actual ones\n",
    "    plot_actual_vs_pred(y_true, y_pred)\n",
    "              \n",
    "    print()\n",
    "    # plotting confusion matrix\n",
    "    plot_confusion_matrix(matrix, ['Failure', 'Success'])\n",
    "    \n",
    "    print()\n",
    "    # plotting ROC curve\n",
    "    plot_roc_curve(y_true, y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer(y_true, y_pred):\n",
    "    \"\"\"Calculates differnt merics for evaluating our models.\"\"\"\n",
    "\n",
    "    threshold = 0.007\n",
    "    threshold_transformed = stats.boxcox(threshold, 0.25)\n",
    "\n",
    "    binary_prediction = y_pred > threshold_transformed\n",
    "    binary_target = y_true > threshold_transformed\n",
    "\n",
    "    \"\"\"Regression metrics\"\"\"\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    explained_variance = explained_variance_score(y_true, y_pred)\n",
    "    r_2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    \"\"\"Classification metrics\"\"\"\n",
    "\n",
    "    # The probability that the model ranks a random\n",
    "    # positive example more highly than a random negative example.\n",
    "    auc = roc_auc_score(binary_target, y_pred)\n",
    "\n",
    "    conf_matrix = confusion_matrix(binary_target, binary_prediction)\n",
    "    # true positives (TP): We predicted Y and they do have the disease.\n",
    "    # true negatives (TN): We predicted N, and they don't have the disease.\n",
    "    # false positives (FP): We predicted Y, but they don't have the disease.\n",
    "    # false negatives (FN): We predicted N, but they do have the disease.\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "    # correct labels\n",
    "    total = tn + fp + fn + tp\n",
    "    actual_yes = fn + tp\n",
    "    actual_no = total - actual_yes\n",
    "\n",
    "    # Overall, how often is the classifier correct?\n",
    "    accuracy = (tp + tn) / total\n",
    "\n",
    "    # Overall, how often is it wrong?\n",
    "    misclassification_rate = (fp + fn) / total\n",
    "\n",
    "    # When it is actually Y, how often does it predict Y?\n",
    "    # Probability that a test result will be positive when the disease is present.\n",
    "    # Recall\n",
    "    tp_rate = tp / actual_yes\n",
    "\n",
    "    # When it is actually N, how often does it predict N?\n",
    "    # Probability that a test result will be negative when the disease is not present.\n",
    "    specificity = tn / actual_no\n",
    "\n",
    "    # When it is actually N, how often does it predict Y?\n",
    "    # 1 - specificity\n",
    "    fp_rate = fp / actual_no\n",
    "\n",
    "    # When it predicts Y, how often is it correct?\n",
    "    # Probability that the disease is present when the test is positive.\n",
    "    # Positive Predictive Value or precision\n",
    "    pp_value = tp / (fp + tp)\n",
    "\n",
    "    # When it predicts N, how often is it correct?\n",
    "    # Probability that the disease is not present when the test is negative.\n",
    "    # Negative Predictive Value\n",
    "    np_value = tn / (tn + fn)\n",
    "\n",
    "    # The weighted average of recall and precision.\n",
    "    f_score = 2 * tp_rate * pp_value / (tp_rate + pp_value)\n",
    "\n",
    "    return {\n",
    "        'rmse': rmse,\n",
    "        'explained_variance': explained_variance,\n",
    "        'r_2': r_2,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'accuracy': accuracy,\n",
    "        'tpr': tp_rate,\n",
    "        'specificity': specificity,\n",
    "        'fpr': fp_rate,\n",
    "        'ppv': pp_value,\n",
    "        'npv': np_value,\n",
    "        'f_score': f_score,\n",
    "        'auc': auc,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5 fold CV strategy: we will validate our models and tune hyperparameters by 5 fold CV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_mean_score(fold_scores):\n",
    "    \"\"\"Calculates model mean score based on list of each folds scores.\"\"\"\n",
    "    if not fold_scores:\n",
    "        return\n",
    "    keys = list(fold_scores[0].keys())\n",
    "    data = {k: [x[k] for x in fold_scores] for k in keys if k != 'confusion_matrix'}\n",
    "    return {k: (np.mean(v), np.std(v)) for k, v in data.items()}   \n",
    "\n",
    "def cv(model, train_data, train_y, n_folds=5, verbose=True):\n",
    "    \"\"\"Helper function for doing cross validation and collecting metrics.\"\"\"\n",
    "\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    training_metrics, validatin_metrics = [], []\n",
    "    for i, (train_ind, valid_ind) in enumerate(kf.split(train_data)):\n",
    "        model_ = clone(model)\n",
    "        model_.fit(train_data.iloc[train_ind], train_y.iloc[train_ind])\n",
    "\n",
    "        train_y_pred = model_.predict(train_data.iloc[train_ind])\n",
    "        t_metrics = scorer(train_y.iloc[train_ind].values, train_y_pred)\n",
    "\n",
    "        valid_y_pred = model_.predict(train_data.iloc[valid_ind])\n",
    "        v_metrics = scorer(train_y.iloc[valid_ind].values, valid_y_pred)\n",
    "        tabular_metrics = [[k, t_metrics[k], v_metrics[k]]\n",
    "                           for k in t_metrics.keys()\n",
    "                           if k != 'confusion_matrix']\n",
    "        \n",
    "        if verbose:\n",
    "            print('\\nfold: {}\\n'.format(i + 1))\n",
    "            print(tabulate(tabular_metrics,\n",
    "                           headers=['metric_name', 'training_set', 'validation_set'],\n",
    "                           tablefmt=\"fancy_grid\",\n",
    "                           floatfmt=\",.3f\"))\n",
    "\n",
    "        training_metrics.append(t_metrics)\n",
    "        validatin_metrics.append(v_metrics)\n",
    "\n",
    "    mean_t_score = cv_mean_score(training_metrics)\n",
    "    mean_v_score = cv_mean_score(validatin_metrics)\n",
    "\n",
    "    tabular_mean_metrics = [\n",
    "        [k, mean_t_score[k][0], mean_t_score[k][1], mean_v_score[k][0], mean_v_score[k][1]]\n",
    "        for k in mean_t_score.keys()]\n",
    "\n",
    "    tabular_mean_metrics = tabulate(\n",
    "        tabular_mean_metrics,\n",
    "        headers=['metric_name', 'train: mean', 'train: std', 'valid: mean', 'valid: std'],\n",
    "        tablefmt=\"fancy_grid\",\n",
    "        floatfmt=\",.3f\")\n",
    "    return tabular_mean_metrics, mean_t_score, mean_v_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data, y, training=True, plot=True, verbose=True):\n",
    "    \"\"\"Evaluates model on train or test data.\"\"\"\n",
    "    \n",
    "    if training:\n",
    "        # fitting model before prediction\n",
    "        model.fit(data, y)\n",
    "    \n",
    "    # predicition\n",
    "    y_pred = model.predict(data)\n",
    "    \n",
    "    metrics = scorer(y, y_pred)\n",
    "    if plot:\n",
    "        plot_model_performance(y, y_pred, metrics.pop('confusion_matrix'))\n",
    "    else:\n",
    "        metrics.pop('confusion_matrix')\n",
    "    if verbose:\n",
    "        for k, v in metrics.items():\n",
    "            print('{}: {:.3f}'.format(k, v))\n",
    "\n",
    "    return model, metrics, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import librairies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train - Test split: we will keep 15% of data for final testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(training_df, test_size=0.15, shuffle=True)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating response variable\n",
    "y_train, X_train = train['Market_Share'], train.drop(columns='Market_Share')\n",
    "y_test, X_test = test['Market_Share'], test.drop(columns='Market_Share')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Base models and their scores on 5 fold CV:**\n",
    "\n",
    "As models we have selected 4 models: Lasso regression, Elastic Net, ExtraTress regression and Gradient tree boosting. First two models are more linear than other two. First 2 models are modifications of linear regession with additional constraints on regression coefficients. The last 2 models are more no-linear models based on decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models, all_models_scores = {}, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **LASSO Regression (Sklearn):**\n",
    "\n",
    "https://en.wikipedia.org/wiki/Lasso_(statistics)\n",
    "\n",
    "https://scikit-learn.org/stable/modules/linear_model.html#lasso\n",
    "\n",
    "This model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's **Robustscaler()** method on pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0003, max_iter=1e3, tol=1e-5, random_state=1))\n",
    "score, _, score_dict = cv(model_lasso, X_train, y_train)\n",
    "\n",
    "all_models['Lasso'] = model_lasso\n",
    "all_models_scores['Lasso'] = score_dict\n",
    "print('\\nLasso score:\\n')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **ElasticNet Regression (Sklearn):**\n",
    "\n",
    "https://en.wikipedia.org/wiki/Elastic_net_regularization\n",
    "\n",
    "https://scikit-learn.org/stable/modules/linear_model.html#elastic-net\n",
    "\n",
    "again made robust to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_enet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=0.8, random_state=3))\n",
    "score, _, score_dict = cv(model_enet, X_train, y_train)\n",
    "\n",
    "all_models['ElasticNet'] = model_enet\n",
    "all_models_scores['ElasticNet'] = score_dict\n",
    "print('\\nElasticNet:\\n')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **ExtraTrees Regressor (Sklearn):**\n",
    "\n",
    "https://www.quora.com/What-is-the-extra-trees-algorithm-in-machine-learning\n",
    "\n",
    "https://scikit-learn.org/stable/modules/ensemble.html#forest\n",
    "\n",
    "https://stats.stackexchange.com/questions/44382/mathematics-behind-classification-and-regression-trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_extratree = ExtraTreesRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=50,\n",
    "    min_samples_split=2, \n",
    "    min_samples_leaf=2,\n",
    "    min_weight_fraction_leaf=0.0002,\n",
    "    max_leaf_nodes=None,\n",
    "    max_features='auto',\n",
    "    min_impurity_decrease=0.00,\n",
    "    random_state=32,\n",
    "    n_jobs=2\n",
    ")\n",
    "\n",
    "score, _, score_dict = cv(model_extratree, X_train, y_train)\n",
    "\n",
    "all_models['ExtraTrees'] = model_extratree\n",
    "all_models_scores['ExtraTrees'] = score_dict\n",
    "print('\\nExtraTrees:\\n')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **GradientTreeBoosting (LightGBM) :**\n",
    "\n",
    "https://en.wikipedia.org/wiki/Gradient_boosting\n",
    "\n",
    "https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d\n",
    "\n",
    "https://lightgbm.readthedocs.io/en/latest/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_lgb = lgb.LGBMRegressor(\n",
    "    boosting_type='gbdt',\n",
    "    objective='rmse',\n",
    "    n_estimators=5000,\n",
    "    learning_rate=0.03,\n",
    "    \n",
    "    max_depth=6,             # Specify the max depth to which tree will grow. \n",
    "    num_leaves=24,           # Number of leaves in one tree\n",
    "\n",
    "    min_child_weight=11,     # minimal sum hessian in one leaf\n",
    "    min_data_in_leaf=20,     # Min number of data in one leaf.\n",
    "    \n",
    "    subsample=0.8,           # Specifies the fraction of data to be used for each iteration\n",
    "    subsample_freq=1,\n",
    "    bagging_seed=9,\n",
    "    \n",
    "    colsample_bytree=0.8,    # Specifies the fraction of features to be taken for each iteration\n",
    "    feature_fraction_seed=7,\n",
    "    \n",
    "    min_gain_to_split=0.002, # Min gain to perform splitting\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=0.0,\n",
    "    n_jobs=2,\n",
    ")\n",
    "score, _, score_dict = cv(model_lgb, X_train, y_train)\n",
    "\n",
    "all_models['GradientTreeBoosting'] = model_lgb\n",
    "all_models_scores['GradientTreeBoosting'] = score_dict\n",
    "print('\\nGradientTreeBoosting:\\n')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean CV scores for each model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names=[]\n",
    "tabular_format = defaultdict(list)\n",
    "for model_name, model_score_dict in all_models_scores.items():\n",
    "    model_names.append(model_name)\n",
    "    for metric_name, metric_score in model_score_dict.items():\n",
    "        tabular_format[metric_name].append(metric_score[0])\n",
    "\n",
    "\n",
    "tabular_format = tabulate([[k] + v for k, v in tabular_format.items()], \n",
    "                          headers=['Metric'] + model_names, floatfmt=\",.3f\",  tablefmt=\"fancy_grid\")\n",
    "print(tabular_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some insights from scores: \n",
    "\n",
    "* As expected **Lasso** and **ElasticNet** performance are almost similar and their performance is low compared to DecisionTree models, which is also expected as we investigated that our data mostly is non linear.\n",
    "\n",
    "* If we compare **GradientTreeBoosting** and **ExtraTrees** then we can notice that their performances are comparable: mean validation RMSE is almost equal and AUC is almost equal. We see some difference for tpr (true positive ratio - When it is actually Y, how often does it predict Y?) and fpr (false positive ratio - When it is actually N, how often does it predict Y?) metrics for given 0.7% success threshold. As we can see both are bigger for GradientTreeBoosting based model **(tpr: 0.413 vs 0.324, fpr: 0.068  vs 0.050)**. So what does it mean? We can assume that GradientTreeBoosting based model learned more from data than ExtraTrees (as it is more complex model than ExtraTrees), so it can differentiate more succesful launches, but together with that the false positive ratio is more by 1.8%, which is not good from bussiness prospective. So GradientTreeBoosting based models pays fpr increase cost (by 1.8%) for having higher ptr by 8.9%. \n",
    "* All models have low tpr, which is a result of having imbalanced dataset (only 19% are successes in dataset).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training scores on full training data for each model:**\n",
    "\n",
    "In this section using all training data we fit model on it and afterwards using the same training data make predictions. We compare that predictions with actual data. So if the model fitted properly, it should show very high performance metrics (AUC > 0.95, RMSE < 0.06)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_fitted_models = {}\n",
    "all_models_predictions = {}\n",
    "for name, model in all_models.items():\n",
    "    print('\\n model: {} train score'.format(name))\n",
    "    fitted_model, _, y_pred = evaluate_model(model, X_train, y_train, training=True, plot=True)\n",
    "    all_fitted_models[name] = fitted_model\n",
    "    all_models_predictions[name] = y_pred\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot all models ROC curves together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.007\n",
    "threshold_transformed = stats.boxcox(threshold, 0.25)\n",
    "binary_target = y_train > threshold_transformed\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for k, y_pred in all_models_predictions.items():\n",
    "    fpr, tpr, _ = roc_curve(binary_target, y_pred)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    label = '{}: ROC (AUC = {:.3f})'.format(k, auc_score)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.6, label=label)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.title('ROC curve on train data')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see *\"linear\"* models **Lasso** and **ElasticNet** are not able to fully fit even on training data. So afterwards we will not consider using them. \n",
    "\n",
    "In contrast,  *\"non linear\"* models fitted with training data almost ideally, even more they have been **overfitted**, that is why our validation scores are not high enough. It is also expected as our data is not big enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature importances for all models:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = pd.Series(all_fitted_models['Lasso']._final_estimator.coef_, index=X_train.columns)\n",
    "feat_imp.sort_values(inplace=True)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.barh(feat_imp.index[:15], feat_imp.values[:15])\n",
    "plt.barh(feat_imp.index[-15:], feat_imp.values[-15:])\n",
    "plt.title('Lassso')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = pd.Series(all_fitted_models['ElasticNet']._final_estimator.coef_, index=X_train.columns)\n",
    "feat_imp.sort_values(inplace=True)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.barh(feat_imp.index[:15], feat_imp.values[:15])\n",
    "plt.barh(feat_imp.index[-15:], feat_imp.values[-15:])\n",
    "plt.title('ElascticNet')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat_imp = pd.Series(all_fitted_models['ExtraTrees'].feature_importances_, index=X_train.columns)\n",
    "feat_imp.nlargest(30).plot(kind='barh', figsize=(8,10))\n",
    "plt.title('ExtraTrees')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = pd.Series(all_fitted_models['GradientTreeBoosting'].feature_importances_, index=X_train.columns)\n",
    "feat_imp.nlargest(30).plot(kind='barh', figsize=(8,10))\n",
    "plt.title('GradientTreeBoosting')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From graphs we can see that different features are important for dfferent models. If we take into consideration only GradiendBoosting and ExtraTrees then we can see for both models there is a common tendency that **RETAILPACKPRICE** related features are important. This is is quite intuitive: in reality market share depends on price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter optimization with GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try to make some parameter optimization. We hope that this will help to overcome overfitting (at least partially)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use AUC as an optimization metric, instead of RMSE\n",
    "auc_scorer = make_scorer(lambda x, y: scorer(x, y)['auc'], greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GradientTreeBoosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://sites.google.com/view/lauraepp/parameters?authuser=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_train, label=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Fix learning rate and number of estimators for tuning tree-based parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.03,\n",
    "    'objective':'rmse',\n",
    "    'max_depth': 6,\n",
    "    'num_leaves': 31,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'subsample_freq': 1,\n",
    "    'bagging_seed': 9,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'feature_fraction_seed': 7,\n",
    "    'n_jobs': 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvresult = lgb.cv(params, lgb_train, num_boost_round=5000, nfold=5, stratified=False, \n",
    "                  metrics=['rmse'], early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_esitimators = int(1.1 * len(cvresult['rmse-mean']))\n",
    "print(n_esitimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* learning_rate=0.03\n",
    "* n_esitimators=850"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Tune max_depth and num_leaves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_search_1 = {\n",
    "    'max_depth': range(3, 8, 1), \n",
    "    'num_leaves': range(7, 255, 8)\n",
    "}\n",
    "gsearch_1 = GridSearchCV(\n",
    "    estimator=lgb.LGBMRegressor(\n",
    "        learning_rate=0.03,\n",
    "        n_estimators=850,\n",
    "        max_depth=6,\n",
    "        num_leaves=31,\n",
    "        min_child_weight=1,\n",
    "        subsample=0.8,\n",
    "        subsample_freq=1,\n",
    "        bagging_seed=9,\n",
    "        colsample_bytree=0.8,\n",
    "        feature_fraction_seed=7,\n",
    "        n_jobs=2),\n",
    "    param_grid=params_search_1,\n",
    "    scoring=auc_scorer,\n",
    "    n_jobs=2,\n",
    "    iid=False,\n",
    "    cv=5)\n",
    "\n",
    "gsearch_1.fit(X_train, y_train)\n",
    "gsearch_1.best_params_, gsearch_1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* learning_rate=0.03\n",
    "* n_estimators=850\n",
    "* max_depth=7\n",
    "* num_leaves=23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Tune min_child_weight and min_child_samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_search_2 = {\n",
    "    'min_child_weight': range(1, 50, 5),\n",
    "    'min_child_samples': range(1, 30, 3)\n",
    "}\n",
    "gsearch_2 = GridSearchCV(\n",
    "    estimator=lgb.LGBMRegressor(\n",
    "        learning_rate=0.03,\n",
    "        n_estimators=850,\n",
    "        max_depth=6,\n",
    "        num_leaves=31,\n",
    "        min_child_weight=1,\n",
    "        subsample=0.8,\n",
    "        subsample_freq=1,\n",
    "        bagging_seed=9,\n",
    "        colsample_bytree=0.8,\n",
    "        feature_fraction_seed=7,\n",
    "        n_jobs=2),\n",
    "    param_grid=params_search_2,\n",
    "    scoring=auc_scorer,\n",
    "    n_jobs=2,\n",
    "    iid=False,\n",
    "    cv=5)\n",
    "\n",
    "gsearch_2.fit(X_train, y_train)\n",
    "gsearch_2.best_params_, gsearch_2.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* learning_rate=0.03\n",
    "* n_estimators=850\n",
    "* max_depth=7\n",
    "* num_leaves=23\n",
    "* min_child_weight=1\n",
    "* min_child_samples=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Tune min_split_gain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_search_3 = {\n",
    "    'min_split_gain': np.linspace(0.001, 0.05, num=25)\n",
    "}\n",
    "\n",
    "gsearch_3 = GridSearchCV(\n",
    "    estimator=lgb.LGBMRegressor(\n",
    "        learning_rate=0.03,\n",
    "        n_estimators=850,\n",
    "        max_depth=7,\n",
    "        num_leaves=23,\n",
    "        min_child_weight=1,\n",
    "        min_child_samples=1,\n",
    "        subsample=0.8,\n",
    "        subsample_freq=1,\n",
    "        bagging_seed=9,\n",
    "        colsample_bytree=0.8,\n",
    "        feature_fraction_seed=7,\n",
    "        min_split_gain=0.0,\n",
    "        n_jobs=2),\n",
    "    param_grid=params_search_3,\n",
    "    scoring=auc_scorer,\n",
    "    n_jobs=2,\n",
    "    iid=False,\n",
    "    cv=5)\n",
    "\n",
    "gsearch_3.fit(X_train, y_train)\n",
    "gsearch_3.best_params_, gsearch_3.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* learning_rate=0.03\n",
    "* n_estimators=850\n",
    "* max_depth=7\n",
    "* num_leaves=23\n",
    "* min_child_weight=1\n",
    "* min_child_samples=1\n",
    "* min_split_gain=0.003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ExtraTrees**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_search_4 = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_features': ['auto', 'log2', 0.8],\n",
    "    'max_depth': [40, 50, 60],\n",
    "    'min_samples_split': [2, 3, 4],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'bootstrap': [True, False],\n",
    "}\n",
    "\n",
    "gsearch_4 = GridSearchCV(\n",
    "    estimator=ExtraTreesRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=50,\n",
    "    min_samples_split=2, \n",
    "    min_samples_leaf=2,\n",
    "    min_weight_fraction_leaf=0.0002,\n",
    "    max_leaf_nodes=None,\n",
    "    max_features='auto',\n",
    "    min_impurity_decrease=0.00,\n",
    "    random_state=32,\n",
    "    n_jobs=2),\n",
    "    param_grid=params_search_4,\n",
    "    scoring=auc_scorer,\n",
    "    n_jobs=2,\n",
    "    iid=False,\n",
    "    cv=5)\n",
    "\n",
    "gsearch_4.fit(X_train, y_train)\n",
    "gsearch_4.best_params_, gsearch_4.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check our models with new parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lgb = lgb.LGBMRegressor(\n",
    "    boosting_type='gbdt',objective='rmse', n_estimators=850,\n",
    "    learning_rate=0.03, max_depth=7, num_leaves=23, min_child_weight=1, min_data_in_leaf=1,    \n",
    "    subsample=0.8, subsample_freq=1, bagging_seed=9, colsample_bytree=0.8, feature_fraction_seed=7,\n",
    "    min_gain_to_split=0.003, n_jobs=2\n",
    ")\n",
    "\n",
    "score, _, _ = cv(final_lgb, X_train, y_train, verbose=False)\n",
    "print('\\nGradientTreeBoosting:\\n')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_extratree = ExtraTreesRegressor(\n",
    "    n_estimators=300, max_depth=50, min_samples_split=4,\n",
    "    min_samples_leaf=1, min_weight_fraction_leaf=0.0002,\n",
    "    max_leaf_nodes=None, max_features='auto', bootstrap=True,\n",
    "    min_impurity_decrease=0.00, random_state=32, n_jobs=2)\n",
    "\n",
    "score, _, _ = cv(final_extratree, X_train, y_train, verbose=False)\n",
    "print('\\nExtraTrees:\\n')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally lets test our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we have to fit using all train data\n",
    "final_lgb.fit(X_train, y_train)\n",
    "final_extratree.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing\n",
    "_, lgb_metrics, lgb_pred = evaluate_model(final_lgb, X_test, y_test, training=False, plot=False, verbose=False)\n",
    "_, ext_metrics, ext_pred = evaluate_model(final_extratree, X_test, y_test, training=False, plot=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "model_names=[]\n",
    "tabular_format = defaultdict(list)\n",
    "for model_name, model_score_dict in [('GradientTreeBoosting', lgb_metrics), ('ExtraTrees', ext_metrics)]:\n",
    "    model_names.append(model_name)\n",
    "    for metric_name, metric_score in model_score_dict.items():\n",
    "        tabular_format[metric_name].append(metric_score)\n",
    "\n",
    "tabular_format = tabulate([[k] + v for k, v in tabular_format.items()], \n",
    "                          headers=['Metric'] + model_names, floatfmt=\",.3f\",  tablefmt=\"fancy_grid\")\n",
    "\n",
    "print('Test results:')\n",
    "print(tabular_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.007\n",
    "threshold_transformed = stats.boxcox(threshold, 0.25)\n",
    "binary_target = y_test > threshold_transformed\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for k, y_pred in [('GradientTreeBoosting', lgb_pred), ('ExtraTrees', ext_pred)]:\n",
    "    fpr, tpr, _ = roc_curve(binary_target, y_pred)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    label = '{}: ROC (AUC = {:.3f})'.format(k, auc_score)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.6, label=label)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.title('ROC curve on test data')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calcualte confidence interval to measure how **confident we are that the true success probability will fall within the range of the interval**.\n",
    "As an observation we should use ppv - positive predictive value, which shows the porbability of having positive sample (aka success), when our model predicts that sample is positive. We will use normal approximation.\n",
    "\n",
    "**ppv ~ P(true success | model predicts success)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppv_conf_interval(ppv, total_p_predicted, confidence):\n",
    "    alpha = 1 - confidence  # target error rate\n",
    "    z = stats.norm.ppf(1 - alpha / 2)  # 1-alpha/2 - quantile of a standard normal distribution\n",
    "    se = z * np.sqrt(ppv * (1 - ppv) / total_p_predicted)  # standard error\n",
    "    return (ppv - se, ppv + se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_total_positives = (lgb_pred > threshold_transformed).sum()\n",
    "extra_tree_total_positives = (ext_pred > threshold_transformed).sum()\n",
    "\n",
    "print('Total positive predictions:')\n",
    "print('GradientTreeBoosting: {}, ExtraTrees: {}'.format(lgb_total_positives, extra_tree_total_positives))\n",
    "\n",
    "tp_lgb = ((lgb_pred > threshold_transformed) * binary_target).sum()\n",
    "tp_ext = ((ext_pred > threshold_transformed) * binary_target).sum()\n",
    "actual_true = sum(binary_target)\n",
    "\n",
    "print('\\nCorrect success predictions:')\n",
    "print('Actual success: {}, GradientTreeBoosting: {}, ExtraTrees: {}'.format(actual_true, tp_lgb, tp_ext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('95% confidence intervals for ppv:')\n",
    "\n",
    "lgb_conf_interval = ppv_conf_interval(lgb_metrics['ppv'], lgb_total_positives, 0.95)\n",
    "extra_tree_conf_interval = ppv_conf_interval(ext_metrics['ppv'], extra_tree_total_positives, 0.95)\n",
    "\n",
    "print('GradientTreeBoosting: [{:.2f}, {:.2f}]'.format(lgb_conf_interval[0], lgb_conf_interval[1]))\n",
    "print('ExtraTrees: [{:.2f}, {:.2f}]'.format(extra_tree_conf_interval[0], extra_tree_conf_interval[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "* **GradientTreeBoosting** is more powerful model than **ExtraTrees** in terms of **tpr (0.473 vs 0.236): probability of predicting success, when it is true success**. \n",
    "* **ExtraTrees** is more powerful model than **GradientTreeBoosting** in terms of **fpr (0.010 vs 0.035): probability of predicting success, when it is true failure**.\n",
    "* As a result of having small **fpr**, **ExtraTrees** has more **positive predictive power (0.86 vs 0.78): probability of being true success, when model predicted success** than **GradientTreeBoosting**. \n",
    "* When **ExtraTrees** says success, then with 0.86 probability and [0.69, 1.04] confidence interval it will be true success.\n",
    "* When **GradientTreeBoosting** says success, then with 0.78 probability and [0.65, 0.93] confidence interval it will be true success.\n",
    "* **ExtraTrees** says success 2 times rarely than **GradientTreeBoosting** in case true success.\n",
    "\n",
    "\n",
    "So what model to select??? \n",
    "\n",
    "One of the models is very conservative and almost do not predicts wrong successes, but it classifies a lot of successes as failures.\n",
    "On the other hand second model in some cases can wrongly classify failure as a success, but it 2 times more correctly classifies successes.\n",
    "\n",
    "The model selection should be calibrated with business needs. If business do not wants to lose money, with possible failures, then we should select **ExtraTrees**, as it in 99% percentage of true failures, predicts failure, but it predicts successs only in 23.6% cases, when it is true success.\n",
    "If business wants to make a bit more risky investments, then we should select **GradientTreeBoosting** (96.5% vs 99%, 47.3% vs 23.6%).\n",
    "\n",
    "\n",
    "As a ML algorithm  **GradientTreeBoosting** is more powerful as it has more **fpr** in case of our imbalanced data (only 19% of whole data is success). So we can say that it learned more from data than **ExtraTrees**. It has quite  higher f_score (0.591 vs 0.371), which is harmonic mean of ppv and sensitivity. This means that as a classificator it does it job much better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test also these models average model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_pred = (lgb_pred + ext_pred) / 2\n",
    "average_metrics = scorer(y_test, average_pred)\n",
    "average_metrics.pop('confusion_matrix')\n",
    "\n",
    "model_names=[]\n",
    "tabular_format = defaultdict(list)\n",
    "for model_name, model_score_dict in [('GradientTreeBoosting', lgb_metrics), ('ExtraTrees', ext_metrics), ('AverageModel', average_metrics )]:\n",
    "    model_names.append(model_name)\n",
    "    for metric_name, metric_score in model_score_dict.items():\n",
    "        tabular_format[metric_name].append(metric_score)\n",
    "\n",
    "tabular_format = tabulate([[k] + v for k, v in tabular_format.items()], \n",
    "                          headers=['Metric'] + model_names, floatfmt=\",.3f\",  tablefmt=\"fancy_grid\")\n",
    "\n",
    "print('Test results:')\n",
    "print(tabular_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see the results are interesting. This model has **fpr=0.010 and ppv=0.913 and tpr=0.382**. So in case of true failure it performs like **ExtraTrees** (almost the same fpr), but in case of true successes its performance much better and close to **GradientTreeBoosting**. Even more its **positive predictive value** is more than for each single model, which is very very good (0.913 vs  0.788 and 0.867). It has also higher auc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_total_positives = (average_pred > threshold_transformed).sum()\n",
    "tp_average = ((average_pred > threshold_transformed) * binary_target).sum()\n",
    "average_conf_interval = ppv_conf_interval(average_metrics['ppv'], average_total_positives, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nCorrect success predictions:')\n",
    "print('Actual success: {}, GradientTreeBoosting: {}, ExtraTrees: {}, AverageModel: {}'.format(\n",
    "    actual_true, tp_lgb, tp_ext, tp_average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('95% confidence intervals for ppv:')\n",
    "print('GradientTreeBoosting: [{:.2f}, {:.2f}]'.format(lgb_conf_interval[0], lgb_conf_interval[1]))\n",
    "print('ExtraTrees: [{:.2f}, {:.2f}]'.format(extra_tree_conf_interval[0], extra_tree_conf_interval[1]))\n",
    "print('AverageModel: [{:.2f}, {:.2f}]'.format(average_conf_interval[0], average_conf_interval[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from bussiness prospective average model is better than each single model: it hase more positive predicive value than **ExtraTrees** and less risky than **GradientTreeBoosting**. So as final model we will take AverageModel for making predictions of our score data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.007\n",
    "threshold_transformed = stats.boxcox(threshold, 0.25)\n",
    "binary_target = y_test > threshold_transformed\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for k, y_pred in [('GradientTreeBoosting', lgb_pred), ('ExtraTrees', ext_pred), ('AverageModel', average_pred)]:\n",
    "    fpr, tpr, _ = roc_curve(binary_target, y_pred)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    label = '{}: ROC (AUC = {:.3f})'.format(k, auc_score)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.6, label=label)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.title('ROC curve on test data')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets find also optimal threshold for our model based on ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(binary_target, average_pred)\n",
    "\n",
    "# The optimal cut off would be where tpr is high and fpr is low\n",
    "# tpr - (1-fpr) is zero or near to zero is the optimal cut off point\n",
    "i = np.arange(len(tpr))  # index for df\n",
    "roc = pd.DataFrame({'fpr': fpr,\n",
    "                    'tpr': tpr,\n",
    "                    '1-fpr': 1 - fpr,\n",
    "                    'tf': tpr - (1 - fpr),\n",
    "                    'thresholds':  thresholds})\n",
    "optimal_threshold = roc.iloc[(roc.tf - 0).abs().argsort()[:1]]\n",
    "\n",
    "# Plot tpr vs 1-fpr\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(roc['tpr'], label='tpr')\n",
    "plt.plot(roc['1-fpr'], color='red', label='1-fpr')\n",
    "plt.xlabel('1-False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.xticks(ticks=[])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('optimal threshold: {:.2f}%'.format(inv_boxcox(optimal_threshold['thresholds'], 0.25).values[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before final prediction we will use all our data for training\n",
    "final_lgb = lgb.LGBMRegressor(\n",
    "    boosting_type='gbdt',\n",
    "    objective='rmse',\n",
    "    n_estimators=850,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=7,\n",
    "    num_leaves=23,\n",
    "    min_child_weight=1,\n",
    "    min_data_in_leaf=1,\n",
    "    subsample=0.8,\n",
    "    subsample_freq=1,\n",
    "    bagging_seed=9,\n",
    "    colsample_bytree=0.8,\n",
    "    feature_fraction_seed=7,\n",
    "    min_gain_to_split=0.003,\n",
    "    n_jobs=2)\n",
    "\n",
    "final_extratree = ExtraTreesRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=50,\n",
    "    min_samples_split=4,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0002,\n",
    "    max_leaf_nodes=None,\n",
    "    max_features='auto',\n",
    "    bootstrap=True,\n",
    "    min_impurity_decrease=0.00,\n",
    "    random_state=32,\n",
    "    n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lgb.fit(training_df.drop(columns='Market_Share'), training_df['Market_Share'])\n",
    "final_extratree.fit(training_df.drop(columns='Market_Share'), training_df['Market_Share']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat_imp = pd.Series(final_extratree.feature_importances_, index=X_train.columns)\n",
    "feat_imp.nlargest(30).plot(kind='barh', figsize=(8,10))\n",
    "plt.title('ExtraTrees')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = pd.Series(final_lgb.feature_importances_, index=X_train.columns)\n",
    "feat_imp.nlargest(30).plot(kind='barh', figsize=(8,10))\n",
    "plt.title('GradientTreeBoosting')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_final_pred = final_lgb.predict(score_df)\n",
    "ext_final_pred = final_extratree.predict(score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_final_pred = (lgb_final_pred + ext_final_pred) / 2\n",
    "average_final_pred = inv_boxcox(average_final_pred, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_score_df = pd.read_csv('score_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_score_df['Market_Share'] = average_final_pred\n",
    "initial_score_df['Success']  = initial_score_df['Market_Share'] > 0.007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_score_df.to_csv('score_data_predicted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
